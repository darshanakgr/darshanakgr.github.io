
@inproceedings{radhakrishnan_erica:_2020,
	address = {Virtual Event Japan},
	title = {{ERICA}: enabling real-time mistake detection \& corrective feedback for free-weights exercises},
	isbn = {9781450375900},
	shorttitle = {{ERICA}},
	url = {https://dl.acm.org/doi/10.1145/3384419.3430732},
	doi = {10.1145/3384419.3430732},
	language = {en},
	urldate = {2023-12-07},
	booktitle = {Proceedings of the 18th {Conference} on {Embedded} {Networked} {Sensor} {Systems}},
	publisher = {ACM},
	author = {Radhakrishnan, Meera and Rathnayake, Darshana and Han, Ong Koon and Hwang, Inseok and Misra, Archan},
	month = nov,
	year = {2020},
	pages = {558--571},
}

@inproceedings{rathnayake_realtime_2018,
	title = {A {Realtime} {Monitoring} {Platform} for {Workflow} {Subroutines}},
	url = {https://ieeexplore.ieee.org/abstract/document/8615557},
	doi = {10.1109/ICTER.2018.8615557},
	abstract = {With the advancement in distributed computing, workflow management systems have shifted towards executing processes on top of distributed and parallel information systems, resulting in distributed workflow management systems. Furthermore, in the present reuse-oriented software engineering era, developers tend to use existing software components or cloud services to build applications rather than developing as all new code. One such requirement in distributed workflow management systems is the monitoring of the progress and state of each of the tasks from a central location. End users should be able to track information of a workflow such as real-time status, anomalies and failures. However, it is challenging to find such platforms or services to remotely observe the progress or the state of a distributed workflow execution. Developers are required to build such monitoring modules from the ground up. In this paper, we attempt to design a cloud service which can provide real-time monitoring for distributed workflow executions. The proposed solution provides a pluggable service module enabling remote monitoring of workflow execution, observation of current state and debugging of the service from a central location. The solution is presented as a simple API, where the developer is only required to invoke methods and publish the state, followed by the reception of such broadcasts from a remote user interface for monitoring. Experiment results suggest the system demonstrates effective real-time monitoring capabilities with improved performance.},
	urldate = {2023-12-07},
	booktitle = {2018 18th {International} {Conference} on {Advances} in {ICT} for {Emerging} {Regions} ({ICTer})},
	author = {Rathnayake, D. and Wickramarachchi, A. and Mallawaarachchi, V. and Meedeniya, D. and Perera, I.},
	month = sep,
	year = {2018},
	note = {ISSN: 2472-7598},
	pages = {41--47},
}

@inproceedings{rathnayake_jointly_2020,
	title = {Jointly {Optimizing} {Sensing} {Pipelines} for {Multimodal} {Mixed} {Reality} {Interaction}},
	url = {https://ieeexplore.ieee.org/abstract/document/9356035/},
	doi = {10.1109/MASS50613.2020.00046},
	abstract = {Natural human interactions for Mixed Reality Applications are overwhelmingly multimodal: humans communicate intent and instructions via a combination of visual, aural and gestural cues. However, supporting low-latency and accurate comprehension of such multimodal instructions (MMI), on resource-constrained wearable devices, remains an open challenge, especially as the state-of-the-art comprehension techniques for each individual modality increasingly utilize complex Deep Neural Network models. We demonstrate the possibility of overcoming the core limitation of latency-vs.-accuracy tradeoff by exploiting cross-modal dependencies-i.e., by compensating for the inferior performance of one model with an increased accuracy of more complex model of a different modality. We present a sensor fusion architecture that performs MMI comprehension in a quasi-synchronous fashion, by fusing visual, speech and gestural input. The architecture is reconfigurable and supports dynamic modification of the complexity of the data processing pipeline for each individual modality in response to contextual changes. Using a representative “classroom” context and a set of four common interaction primitives, we then demonstrate how the choices between low and high complexity models for each individual modality are coupled. In particular, we show that (a) a judicious combination of low and high complexity models across modalities can offer a dramatic 3-fold decrease in comprehension latency together with an increase 10-15\% in accuracy, and (b) the right collective choice of models is context dependent, with the performance of some model combinations being significantly more sensitive to changes in scene context or choice of interaction.},
	urldate = {2023-12-07},
	booktitle = {2020 {IEEE} 17th {International} {Conference} on {Mobile} {Ad} {Hoc} and {Sensor} {Systems} ({MASS})},
	author = {Rathnayake, Darshana and de Silva, Ashen and Puwakdandawa, Dasun and Meegahapola, Lakmal and Misra, Archan and Perera, Indika},
	month = dec,
	year = {2020},
	note = {ISSN: 2155-6814},
	pages = {309--317},
}

@inproceedings{rathnayake_liloc:_2023,
	address = {New York, NY, USA},
	series = {{IoTDI} '23},
	title = {{LILOC}: {Enabling} {Precise} {3D} {Localization} in {Dynamic} {Indoor} {Environments} using {LiDARs}},
	isbn = {9798400700378},
	shorttitle = {{LILOC}},
	url = {https://doi.org/10.1145/3576842.3582364},
	doi = {10.1145/3576842.3582364},
	abstract = {We present LiLoc, a system for precise 3D localization and tracking of mobile IoT devices (e.g., robots) in indoor environments using multi-perspective LiDAR sensing. The key differentiators in our work are: (a) First, unlike traditional localization approaches, our approach is robust to dynamically changing environmental conditions (e.g., varying crowd levels, object placement/layout changes); (b) Second, unlike prior work on visual and 3D SLAM, LiLoc is not dependent on a pre-built static map of the environment and instead works by utilizing dynamically updated point clouds captured from both infrastructural-mounted LiDARs and LiDARs equipped on individual mobile IoT devices. To achieve fine-grained, near real-time location tracking, it employs complex 3D ‘global’ registration among the two point clouds only intermittently to obtain robust spot location estimates and further augments it with repeated simpler ‘local’ registrations to update the trajectory of IoT device continuously. We demonstrate that LiLoc can (a) support accurate location tracking with location and pose estimation error being {\textless}=7.4cm and {\textless}=3.2° respectively for 84\% of the time and the median error increasing only marginally (8\%), for correctly estimated trajectories, when the ambient environment is dynamic, (b) achieve a 36\% reduction in median location estimation error compared to an approach that uses only quasi-static global point cloud, and (c) obtain spot location estimates with a latency of only 973 msecs.},
	urldate = {2023-12-07},
	booktitle = {Proceedings of the 8th {ACM}/{IEEE} {Conference} on {Internet} of {Things} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Rathnayake, Darshana and Radhakrishnan, Meeralakshmi and Hwang, Inseok and Misra, Archan},
	month = may,
	year = {2023},
	keywords = {LiDAR, Pose Estimation, Trajectory Tracking, 3D Localization, Dynamic Indoor Environments},
	pages = {158--171},
}
